{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpGgW5kRadp6"
      },
      "source": [
        "# Hacking Into FasterRcnn in Pytorch\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [jupyter]\n",
        "- image: images/chart-preview.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxV-Y8VXadp_"
      },
      "source": [
        "# Brief Intro\n",
        "In the post I will show how to tweak some of the internals of FaterRcnn in Pytorch. I am assuming the reader is someone who already have trained an object detection model using pytorch. If not there is and excellent tutorial in [pytorch website](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShiEuz2FadqA"
      },
      "source": [
        "## Small Insight into the model\n",
        "\n",
        "Basically Faster Rcnn is a two stage detector\n",
        "1. The first stage is the Region proposal network which is resposible for knowing the objectness and corresponding bounding boxes. So essentially the RegionProposalNetwork will give the proposals of whether and object is there or not\n",
        "2. These proposals will be used by the RoIHeads which outputs the detections .\n",
        "    * Inside the RoIHeads roi align is done\n",
        "    * There will be a box head and box predictor\n",
        "    * The losses for the predictions\n",
        "3. In this post i will try to show how we can add custom parts to the torchvision FasterRcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gc-h3GOmadqB",
        "outputId": "321b08bd-bc0e-4228-f7af-402da71867e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version 1.11.0\n",
            "torchvision version 0.12.0\n"
          ]
        }
      ],
      "source": [
        "#collapse-hide\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(f'torch version {torch.__version__}')\n",
        "print(f'torchvision version {torchvision.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JeVXS67adqD"
      },
      "source": [
        "# Custom Backone\n",
        "1. The backbone can be without FeaturePyramidNetwork\n",
        "2. With FeaturePyramidNetwork"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPE1lSgpadqE"
      },
      "source": [
        "## Custom Backbone without FPN\n",
        "This is pretty well written in the pytorch tutorials section, i will add some comments to it additionally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knqf04aPadqE"
      },
      "outputs": [],
      "source": [
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "#we need to specify an outchannel of this backone specifically because this outchannel will be\n",
        "#used as an inchannel for the RPNHEAD which is producing the out of RegionProposalNetwork\n",
        "#we can know the number of outchannels by looking into the backbone \"backbone??\"\n",
        "backbone.out_channels = 1280\n",
        "#by default the achor generator FasterRcnn assign will be for a FPN backone, so\n",
        "#we need to specify a  different anchor generator\n",
        "anchor_generator = AnchorGenerator(sizes=((128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "#here at each position in the grid there will be 3x3=9 anchors\n",
        "#and if our backbone is not FPN then the forward method will assign the name '0' to feature map\n",
        "#so we need to specify '0 as feature map name'\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                 output_size=9,\n",
        "                                            sampling_ratio=2)\n",
        "#the output size is the output shape of the roi pooled features which will be used by the box head\n",
        "model = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDrMhwTFadqF"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 600)]\n",
        "predictions = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlSF0Q1NadqG"
      },
      "source": [
        "## Custom Backbone with FPN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXsq1BpvadqH"
      },
      "source": [
        "The Resnet50Fpn available in torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD9FVm80adqH"
      },
      "outputs": [],
      "source": [
        "# load a model pre-trained pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZC79LvyadqI"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7qlxWS2adqI"
      },
      "source": [
        "### Adding a different resenet backbone\n",
        "1. Just change to a different resenet\n",
        "1. Shows how we should change roi_pooler and anchor_generator along with the backbone changes if we are not using all the layers from FPN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgdp_pxXadqI"
      },
      "source": [
        "### Using all layers from FPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft3Oh5kGadqJ"
      },
      "outputs": [],
      "source": [
        "#hte returned layers are layer1,layer2,layer3,layer4 in returned_layers\n",
        "backbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone('resnet101',pretrained=True)\n",
        "model = FasterRCNN(backbone,num_classes=2)                                                                       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRbTMO2IadqJ"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJT5WYgxadqJ"
      },
      "source": [
        "### Using not all layers from FPN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiNeM-QwadqK"
      },
      "source": [
        "The size of the last fature map in a Resnet50.Later i will show the sizes of the feature maps we use when we use FPN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2ehO64HadqK",
        "outputId": "bc71d1fa-01d1-46d3-dbcf-985522aa4088"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2048, 13, 13])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#collapse-hide\n",
        "#just to show what will be out of of a normal resnet without fpn\n",
        "res = torchvision.models.resnet50()\n",
        "pure = nn.Sequential(*list(res.children())[:-2])\n",
        "temp = torch.rand(1,3,400,400)\n",
        "pure(temp).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkpcgRQQadqK"
      },
      "source": [
        "The required layers can be obtained by specifying the returned layers parameters.Also the resnet backbone of different depth can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqJxK1ubadqK"
      },
      "outputs": [],
      "source": [
        "#the returned layers are layer1,layer2,layer3,layer4 in returned_layers\n",
        "backbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone('resnet101',pretrained=True,\n",
        "                                                                          returned_layers=[2,3,4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3VbKdVradqL"
      },
      "source": [
        "Here we are using feature maps of the following shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v63VrUf9adqL",
        "outputId": "b450ed65-824a-4955-e61c-df73c7e59205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    torch.Size([1, 256, 50, 50])\n",
            "1    torch.Size([1, 256, 25, 25])\n",
            "2    torch.Size([1, 256, 13, 13])\n",
            "pool    torch.Size([1, 256, 7, 7])\n"
          ]
        }
      ],
      "source": [
        "#collapse-hide\n",
        "out = backbone(temp)\n",
        "for i in out.keys():\n",
        "    print(i,'  ',out[i].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxKTrGRFadqL"
      },
      "outputs": [],
      "source": [
        "#from the above we can see that the feature are feat maps should be 0,1,2,pool\n",
        "#where pool comes from the default extra block\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0','1','2','pool'],\n",
        "                output_size=7,\n",
        "                sampling_ratio=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awmpTcokadqL"
      },
      "source": [
        "So essentially what we did was we selected the last three layers in FPN by specifying them in the returned layers, by default, the backbone will add a pool layer on top of the last layer. So we are left with four layers. Now the RoIAlign need to be done in these four layers. If we dnt specify the RoIAlign it will use the by default assume we have used all layers from FPN in torchvision. So we need to specifically give the feauture maps that we used. The usage of feature maps can be our application specific, some time you might need to detect small objects sometimes the object of interest will be large objects only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoiNdwcAadqL"
      },
      "outputs": [],
      "source": [
        "#we will need to give anchor_generator because the deafault anchor generator assumes we use all layers in fpn \n",
        "#since we have four layers in fpn here we need to specify 4 anchors\n",
        "anchor_sizes = ((32), (64), (128),(256) ) \n",
        "aspect_ratios = ((0.5,1.0, 1.5,2.0,)) * len(anchor_sizes)\n",
        "anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Y_rfJSadqM"
      },
      "source": [
        "Since we have four layers in our FPN we need to specify the anchors. So here each feature map will have 4 anchors at each position.So the first feature map will have anchor size 32 and four of them will be there at each position in the feature map of aspect_ratios (0.5,1.0, 1.5,2.0). Now we can pass these to the FasterRCNN class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP07V3OnadqM"
      },
      "outputs": [],
      "source": [
        "model = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator,box_roi_pool=roi_pooler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NafA1coyadqM"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7ExlnKaadqM"
      },
      "source": [
        "# Custom Predictor\n",
        "The predictor is what that outputs the classes and the corresponding  bboxes . By default these have two layers one for class and one for bboxes,but we can add more before it if we want to,so if you have a ton of data this might come handy,(remember there is already a box head before the predictor head, so you might not need this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeO3rHuFadqM"
      },
      "outputs": [],
      "source": [
        "class Custom_predictor(nn.Module):\n",
        "    def __init__(self,in_channels,num_classes):\n",
        "        super(Custom_predictor,self).__init__()\n",
        "        self.additional_layer = nn.Linear(in_channels,in_channels) #this is the additional layer  \n",
        "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
        "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
        "        \n",
        "        \n",
        "    def forward(self,x):\n",
        "        if x.dim() == 4:\n",
        "            assert list(x.shape[2:]) == [1, 1]\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.additional_layer(x)\n",
        "        scores = self.cls_score(x)\n",
        "        bbox_deltas = self.bbox_pred(x)\n",
        "        return scores, bbox_deltas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6f6N_KEadqN"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "#we need the out channels of the box head to pass tpp custom predictor\n",
        "in_features = model.roi_heads.box_head.fc7.out_features\n",
        "#now we can add the custom predictor to the model\n",
        "num_classes =2\n",
        "model.roi_heads.box_predictor = Custom_predictor(in_features,num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGpDy-GzadqN"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2sgziRjadqN"
      },
      "source": [
        "# Custom BoxHead\n",
        "The ouptuts of the roi_align are first passed through the box head before they are passed to the Predictor, there are two linear layers and we can customize them as we want, be careful with the dimensions since they can break the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucusAV3adqN"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhKAb6h6adqP"
      },
      "outputs": [],
      "source": [
        "class CustomHead(nn.Module):\n",
        "    def __init__(self,in_channels,roi_outshape,representation_size):\n",
        "        super(CustomHead,self).__init__()\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels,in_channels,kernel_size=3,padding=1)#this is teh additional layer adde\n",
        "        #we will be sending a flattened layer, the size will eb in_channels*w*h, here roi_outshape represents it\n",
        "        \n",
        "        self.fc6 = nn.Linear(in_channels*roi_outshape**2, representation_size)\n",
        "        self.fc7 = nn.Linear(representation_size, representation_size)\n",
        "        \n",
        "    def forward(self,x):\n",
        "       # breakpoint()\n",
        "        \n",
        "        x = self.conv(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        import torch.nn.functional as F\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.relu(self.fc7(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztNOvCtOadqQ"
      },
      "source": [
        "1. We need in_channels and representation size, remember the output of this is the input of box_predictor, so we can get the representation size of box_head from the input of box_predictor.\n",
        "2. The in_channels can be got from the backbone out channels.\n",
        "3. After the flattening the width and height also need to be considered which we wil get from roi_pool output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqyw86uhadqQ"
      },
      "outputs": [],
      "source": [
        "in_channels = model.backbone.out_channels \n",
        "roi_outshape = model.roi_heads.box_roi_pool.output_size[0]\n",
        "representation_size=model.roi_heads.box_predictor.cls_score.in_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DAnJd1sadqQ"
      },
      "outputs": [],
      "source": [
        "model.roi_heads.box_head  = CustomHead(in_channels,roi_outshape,representation_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV_P_yVdadqQ"
      },
      "outputs": [],
      "source": [
        "num_classes=2\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(representation_size, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWJZ32c7adqR"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpM5CkQjadqR"
      },
      "source": [
        "# CustomLoss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLlxndUhadqR"
      },
      "source": [
        "This is the modification for loss of FasterRcnn Predictor.\n",
        "1. You can modify the loss by defining the fastrcnn_loss and making chages where you want.\n",
        "2. Then pass as say model.roi_heads.fastrcnn_loss = Custom_loss\n",
        "3. Usually we replace the F.crossentropy loss by say Focal loss or label smoothing loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9W_xFXKadqR"
      },
      "outputs": [],
      "source": [
        "import torchvision.models.detection._utils as det_utils\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B7-pUT1adqS"
      },
      "source": [
        "The below loss function is taken from [Aman Aroras blog](https://amaarora.github.io/2020/07/18/label-smoothing.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPzvgk6HadqS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Helper functions from fastai\n",
        "def reduce_loss(loss, reduction='mean'):\n",
        "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
        "\n",
        "\n",
        "# Implementation from fastai https://github.com/fastai/fastai2/blob/master/fastai2/layers.py#L338\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, ε:float=0.1, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.ε,self.reduction = ε,reduction\n",
        "    \n",
        "    def forward(self, output, target):\n",
        "        # number of classes\n",
        "        c = output.size()[-1]\n",
        "        log_preds = F.log_softmax(output, dim=-1)\n",
        "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
        "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
        "        # (1-ε)* H(q,p) + ε*H(u,p)\n",
        "        return (1-self.ε)*nll + self.ε*(loss/c) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE9ZTOJIadqS"
      },
      "outputs": [],
      "source": [
        "custom_loss = LabelSmoothingCrossEntropy()\n",
        "#torchvision.models.detection.roi_heads.fastrcnn_loss??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8DMTIaQadqS"
      },
      "outputs": [],
      "source": [
        "def custom_fastrcnn_loss(class_logits, box_regression, labels, regression_targets):\n",
        "    # type: (Tensor, Tensor, List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]\n",
        "    \"\"\"\n",
        "    Computes the loss for Faster R-CNN.\n",
        "\n",
        "    Arguments:\n",
        "        class_logits (Tensor)\n",
        "        box_regression (Tensor)\n",
        "        labels (list[BoxList])\n",
        "        regression_targets (Tensor)\n",
        "\n",
        "    Returns:\n",
        "        classification_loss (Tensor)\n",
        "        box_loss (Tensor)\n",
        "    \"\"\"\n",
        "    \n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    regression_targets = torch.cat(regression_targets, dim=0)\n",
        "\n",
        "    classification_loss = custom_loss(class_logits, labels) #ADDING THE CUSTOM LOSS HERE\n",
        "\n",
        "    # get indices that correspond to the regression targets for\n",
        "    # the corresponding ground truth labels, to be used with\n",
        "    # advanced indexing\n",
        "    sampled_pos_inds_subset = torch.where(labels > 0)[0]\n",
        "    labels_pos = labels[sampled_pos_inds_subset]\n",
        "    N, num_classes = class_logits.shape\n",
        "    box_regression = box_regression.reshape(N, -1, 4)\n",
        "\n",
        "    box_loss = det_utils.smooth_l1_loss(\n",
        "        box_regression[sampled_pos_inds_subset, labels_pos],\n",
        "        regression_targets[sampled_pos_inds_subset],\n",
        "        beta=1 / 9,\n",
        "        size_average=False,\n",
        "    )\n",
        "    box_loss = box_loss / labels.numel()\n",
        "\n",
        "    return classification_loss, box_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcOr2uqiadqT"
      },
      "source": [
        "# Note on how to vary the anchor generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LullBUh_adqT"
      },
      "source": [
        "The way in which anchor generators are assigned when we use backbone with and without fpn is different. When we are not using FPN there will be only one feature map and for that feature map we need to specify anchors of different shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy1GgBU3adqT"
      },
      "outputs": [],
      "source": [
        "anchor_generator = AnchorGenerator(sizes=((128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn1MwJYiadqT"
      },
      "source": [
        "In the above case suppose we have a feature map of shape 7x7, then at each cell in it there will be 9 anchors,three each of shapes 128,256 and 512,with the corresponding aspect rations. But when we are using FPN we have different feature maps, so its more effective we use different feature maps for different layers. Small sized objects are deteted using the earlier feature maps and thus for those we can specify a small sized anchor say 32 and for the later layers we can specify larger anchors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvlW9vTzadqT"
      },
      "outputs": [],
      "source": [
        "anchor_sizes = ((32), (64), (128),(256) ) \n",
        "aspect_ratios = ((0.5,1.0, 1.5,2.0,)) * len(anchor_sizes)\n",
        "anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViOCSSp_adqT"
      },
      "source": [
        "In the above i am using the same aspect ratio for all the sizes so i am just multiplying by the lenght of the anchor_sizes, but if we want to specify different aspect ratios its totally possible. But be carefull to specifiy the same number of aspect ratios for each anchor sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3OL1NrKadqU"
      },
      "source": [
        "# Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0QlL5k4adqU"
      },
      "source": [
        "All the above hacks are just modification of the existing wonderful torchvision library."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2020-12-19-Hacking_fasterRcnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "aab0ec0e679e9c9fe5e8a1739a642d8456688678f7924db8d58b62c9618a230e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
