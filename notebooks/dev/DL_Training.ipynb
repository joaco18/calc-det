{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "from pathlib import Path\n",
    "import os\n",
    "repo_path = Path.cwd()/'drive/MyDrive/calcification-detection-project/calcification_detecion/calc-det/notebooks/'\n",
    "os.chdir(str(repo_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /content/drive/MyDrive/calcification-detection-project/calcification_detecion/data_rois.zip /home/\n",
    "!unzip /home/data_rois.zip -d /home\n",
    "!mv /home/home/vzalevskyi/projects/data_rois /home/data_rois\n",
    "!rm -r /home/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "thispath = Path.cwd().resolve()\n",
    "import sys; sys.path.insert(0, str(thispath.parent))\n",
    "\n",
    "from deep_learning.dataset.dataset import INBreast_Dataset_pytorch\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from general_utils.plots import simple_im_show, simple_im_show2\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, f1_score, roc_auc_score, accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "\n",
    "def sensivity_specifity_cutoff(y_true: np.ndarray, y_score: np.ndarray):\n",
    "    '''Finds data-driven cut-off for classification\n",
    "    Cut-off is determied using Youden's index defined as sensitivity + specificity - 1.\n",
    "    Args:\n",
    "      y_true (np.ndarray): True binary labels.\n",
    "      y_score (np.ndarray): Target scores.\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    idx = np.argmax(tpr - fpr)\n",
    "    return thresholds[idx]\n",
    "\n",
    "\n",
    "def get_metrics(labels, preds):\n",
    "    th = sensivity_specifity_cutoff(labels, preds)\n",
    "    bin_preds = np.where(preds > th, True, False)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, bin_preds).ravel()\n",
    "    \n",
    "    return {'auroc': roc_auc_score(labels, preds),\n",
    "            'f1_score': f1_score(labels, bin_preds),\n",
    "            'accuracy': (tp+tn)/(tp+tn+fp+fn),\n",
    "            'precision': tp/(tp+fp),\n",
    "            'sensitivity': tp/(tp+fn),\n",
    "            'specificity': tn/(tn+fp),\n",
    "            'threshold': th\n",
    "            }\n",
    "\n",
    "\n",
    "def tensorboard_logs(writer, epoch_loss, epoch, metrics, phase):\n",
    "    writer.add_scalar(f\"Loss/{phase}\", epoch_loss, epoch)\n",
    "    writer.add_scalar(f\"Accuracy/{phase}\", metrics['accuracy'], epoch)\n",
    "    writer.add_scalar(f\"F1_score/{phase}\", metrics['f1_score'], epoch)\n",
    "    writer.add_scalar(f\"Auroc/{phase}\", metrics['auroc'], epoch)\n",
    "    writer.add_scalar(f\"Sensitivity/{phase}\", metrics['sensitivity'], epoch)\n",
    "    writer.add_scalar(f\"Specificity/{phase}\", metrics['specificity'], epoch)\n",
    "    writer.add_scalar(f\"Precision/{phase}\", metrics['precision'], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vzalevskyi/projects/calc-det/deep_learning/dataset/dataset.py:38: DtypeWarning: Columns (16,17) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  super(INBreast_Dataset_pytorch, self).__init__(\n"
     ]
    }
   ],
   "source": [
    "settings = {\n",
    "    'activation': 'leaky_relu',\n",
    "    'dropout': 0.2,\n",
    "    'fc_dims': (512, 512),\n",
    "    'freeze_weights': False,\n",
    "    'backbone': 'resnet18',\n",
    "    'pretrained': True,\n",
    "    'criterion': 'bce_with_logits',\n",
    "    'lr': 0.0001,\n",
    "    'optim': 'sgd',\n",
    "    'momentum': 0.9,\n",
    "    'lr_scheduler': 'steplr_sz_10_g_0.1',\n",
    "    'n_epochs': 3,\n",
    "    'experiment_name': 'resnet18',\n",
    "    'transforms': False\n",
    "}\n",
    "\n",
    "\n",
    "# CHANGEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "dataset_arguments = {\n",
    "    'extract_patches': False, 'delete_previous': False,\n",
    "    'extract_patches_method': 'all', 'patch_size': 224, 'stride': 100,\n",
    "    'min_breast_fraction_roi': 0.5, 'n_jobs': -1, 'cropped_imgs': True,\n",
    "    'ignore_diameter_px': 15, 'patch_images_path': Path('/home/vzalevskyi/projects/data_rois/') # FOR GDRIVE '/home/data_rois/'\n",
    "}\n",
    "# CHANGEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "settings['dataset_arguments'] = dataset_arguments\n",
    "settings['neg_to_pos_ratio'] = 1\n",
    "\n",
    "image_datasets = {\n",
    "    'train': INBreast_Dataset_pytorch(\n",
    "        partitions=['train'],\n",
    "        neg_to_pos_ratio=settings['neg_to_pos_ratio'],\n",
    "        balancing_seed=0,\n",
    "        **dataset_arguments\n",
    "    ),\n",
    "    'val': INBreast_Dataset_pytorch(\n",
    "        partitions=['validation'],\n",
    "        neg_to_pos_ratio=None,\n",
    "        **dataset_arguments\n",
    "    )\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'val': DataLoader(\n",
    "        image_datasets['val'],\n",
    "        batch_size=128,\n",
    "        num_workers=4,\n",
    "        drop_last=False\n",
    "    ),\n",
    "    'train': DataLoader(\n",
    "        image_datasets['train'], \n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        drop_last=False\n",
    "    )\n",
    "}\n",
    "\n",
    "data_transforms = {\n",
    "    'train': None,\n",
    "    'val': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, experiment_name, num_epochs=30):\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    # Guarantee reproducibility\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(1442)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Holders for best model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    # Tensorboard loggs\n",
    "    log_dir = \\\n",
    "        Path.cwd().parent.parent/f'data/deepl_runs/{experiment_name}/tensorboard'\n",
    "    log_dir.mkdir(exist_ok=True, parents=True)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        image_datasets['train'].update_sample_used(epoch)\n",
    "        dataloaders['train'] = DataLoader(\n",
    "            image_datasets['train'], \n",
    "            batch_size=16,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                if epoch != 0:\n",
    "                    scheduler.step()\n",
    "                writer.add_scalar(\n",
    "                    f\"LearningRate/{phase}\", scheduler.get_last_lr()[0], epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            # Holders for losses, preds and labels\n",
    "            running_loss = 0.0\n",
    "            epoch_preds = []\n",
    "            epoch_labels = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for it, sample in tqdm(\n",
    "                  enumerate(dataloaders[phase]), total=len(dataloaders[phase])):\n",
    "                \n",
    "                # WE WANT TO TRANSFORMATIONS!!\n",
    "                # Apply transformations and send to device\n",
    "                # sample['img'] = data_transforms[phase](sample['img'])\n",
    "                \n",
    "                \n",
    "                inputs = sample['img'].to(device)\n",
    "                labels = sample['label'].to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    epoch_preds.append(np.asarray(\n",
    "                        torch.sigmoid(outputs.detach()).flatten().cpu()))\n",
    "                    epoch_labels.append(np.asarray(labels.detach().cpu()))\n",
    "                    \n",
    "                    loss = criterion(outputs.flatten(), labels.float())\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                if it in [25, 50, 100]:\n",
    "                    writer.add_images(\n",
    "                        f'Images/{phase}', sample['img'].cpu(), epoch)\n",
    "                # Get the loss itertively\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Compute the metrics for the epoch\n",
    "            epoch_preds = np.concatenate(epoch_preds)\n",
    "            epoch_labels = np.concatenate(epoch_labels)\n",
    "            epoch_loss = running_loss / len(epoch_preds)\n",
    "            \n",
    "            metrics = get_metrics(epoch_labels, epoch_preds)\n",
    "            tensorboard_logs(writer, epoch_loss, epoch, metrics, phase)\n",
    "            \n",
    "            epoch_acc = metrics['accuracy']\n",
    "            epoch_f1 = metrics['f1_score']\n",
    "            epoch_auroc = metrics['auroc']\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}' \\\n",
    "                  f' F1: {epoch_f1:.4f} AUROC: {epoch_auroc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_threshold = metrics['threshold']\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {(time_elapsed // 60):.0f}m ' \\\n",
    "          f'{(time_elapsed % 60):.0f}s')\n",
    "    print(f'Best val F1 score: {best_f1:4f}, threshold {best_threshold}')\n",
    "    \n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # load best model weights\n",
    "    models_path = \\\n",
    "        Path.cwd().parent.parent/f'data/deepl_runs/{experiment_name}'\n",
    "    models_path.mkdir(exist_ok=True, parents=True)\n",
    "    with open(str(models_path/f'{experiment_name}.p'), 'wb') as f:\n",
    "        pickle.dump(best_model_wts, f)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "class CNNClasssifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation: nn.Module = nn.LeakyReLU(),\n",
    "        dropout: float = 0.5,\n",
    "        fc_dims: tuple = (512, 512),\n",
    "        freeze_weights: bool = False,\n",
    "        backbone: str = 'resnet18',\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        self.model = getattr(models, backbone)\n",
    "        if pretrained:\n",
    "            self.model = self.model(pretrained=pretrained)\n",
    "        else:\n",
    "            self.model = self.model()\n",
    "\n",
    "        if freeze_weights:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if hasattr(self.model, 'fc'):\n",
    "            n_inputs = self.model.fc.in_features\n",
    "        else:\n",
    "            n_inputs = self.model.classifier[0].in_features\n",
    "        classifier = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(n_inputs, fc_dims[0])),\n",
    "            ('act1', activation),\n",
    "            ('do1', nn.Dropout(dropout)),\n",
    "            ('fc2', nn.Linear(fc_dims[0], fc_dims[1])),\n",
    "            ('act2', activation),\n",
    "            ('do2', nn.Dropout(dropout)),\n",
    "            ('fc3', nn.Linear(fc_dims[1], 1))\n",
    "        ]))\n",
    "\n",
    "        if hasattr(self.model, 'fc'):\n",
    "          self.model.fc = classifier\n",
    "        else:\n",
    "          self.model.classifier = classifier\n",
    "\n",
    "        self.model.apply(self.initialize_weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_weights(m):\n",
    "        # if isinstance(m, nn.Conv2d):\n",
    "        #     nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "        #     if m.bias is not None:\n",
    "        #         nn.init.constant_(m.bias.data, 0)\n",
    "        # elif isinstance(m, nn.BatchNorm2d):\n",
    "        #     nn.init.constant_(m.weight.data, 1)\n",
    "        #     nn.init.constant_(m.bias.data, 0)\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight.data)\n",
    "            nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings['experiment_name'] = 'resnet_overfit_02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/547 [00:05<15:13,  1.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1142051/1266761965.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m model_ft = train_model(model, criterion, optimizer_conv, exp_lr_scheduler,\n\u001b[0m\u001b[1;32m     26\u001b[0m                        settings['experiment_name'], settings['n_epochs'])\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1142051/518537542.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, experiment_name, num_epochs)\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/calc_det/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/calc_det/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "activation = nn.LeakyReLU() if settings['activation'] == 'leaky_relu' else None\n",
    "\n",
    "cnn = CNNClasssifier(\n",
    "    activation=activation,\n",
    "    dropout=settings['dropout'],\n",
    "    fc_dims=settings['fc_dims'],\n",
    "    freeze_weights=settings['freeze_weights'],\n",
    "    backbone=settings['backbone'],\n",
    "    pretrained=settings['pretrained'],\n",
    ")\n",
    "\n",
    "model = cnn.model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opoosed to before.\n",
    "optimizer_conv = optim.SGD(\n",
    "    model.parameters(), lr=settings['lr'], momentum=settings['momentum'])\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=10, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       settings['experiment_name'], settings['n_epochs'])\n",
    "\n",
    "store_path = \\\n",
    "    Path.cwd().parent.parent/f'data/deepl_runs/{settings[\"experiment_name\"]}'\n",
    "with open(store_path/'cofig.p', 'wb') as f:\n",
    "    pickle.dump(settings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [00:05<00:00, 25.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over data.\n",
    "pos_imgs = 0\n",
    "tot_imgs = 0\n",
    "for it, sample in tqdm(enumerate(dataloaders['train']), total=len(dataloaders['train'])):\n",
    "    \n",
    "    # print(sample.keys())\n",
    "    # print(sample['label'])\n",
    "    # print(sample['img'][0].shape)\n",
    "    \n",
    "    # break\n",
    "    pos_imgs+=sample['label'].sum()\n",
    "    tot_imgs+=len(sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/calcification-detection-project/calcification_detecion/data/deepl_runs/resnet18/tensorboard "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('calc_det')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d785cc99391f1decae0a31664a2bc430e1bb802c6f8076d9d1b38f05ab7f6160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
