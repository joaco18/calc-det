{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5894d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys; sys.path.insert(0, '..')\n",
    "from database.dataset import *\n",
    "from models.bria2014.feature_extraction import feature_instantiator\n",
    "from mc_candidate_proposal.hough_mc import HoughCalcificationDetection\n",
    "from metrics.metrics import get_tp_fp_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a6fc9",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f7d3e",
   "metadata": {},
   "source": [
    "#### Check mayimum value in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d075b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcm_path = Path('/home/jseia/Desktop/ml-dl/calc-det/data/INbreast Release 1.0/AllDICOMs/')\n",
    "\n",
    "# max_val = []\n",
    "# for dcm in dcm_path.iterdir():\n",
    "#     if dcm.name.endswith('.dcm'):\n",
    "#         img = sitk.ReadImage(str(dcm))\n",
    "#         img_array = sitk.GetArrayFromImage(img)\n",
    "#         max_val.append(img_array.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08e5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcm_path = Path('/home/jseia/Desktop/ml-dl/calc-det/data/INbreast Release 1.0/AllDICOMs/')\n",
    "\n",
    "# bits_alloc = []\n",
    "# bits_stored = []\n",
    "# for dcm in dcm_path.iterdir():\n",
    "#     if dcm.name.endswith('.dcm'):\n",
    "#         img = pydicom.dcmread(str(dcm), stop_before_piyels=True)\n",
    "#         bits_alloc.append(img['BitsAllocated'].value)\n",
    "#         bits_stored.append(img['BitsStored'].value)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b015dde",
   "metadata": {},
   "source": [
    "#### Quatum noise tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4247ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def T(y, y_max=4095, T_max=65.535):\n",
    "#     return T_max*np.sqrt(y)/np.sqrt(y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c129d",
   "metadata": {},
   "source": [
    "### Detection stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2969bed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys; sys.path.insert(0, '..')\n",
    "\n",
    "from database.dataset import INBreast_Dataset\n",
    "from mc_candidate_proposal.hough_mc import HoughCalcificationDetection\n",
    "from models.bria2014.haar_extractor import HaarFeatureExtractor\n",
    "from metrics.metrics import get_tp_fp_fn\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "db = INBreast_Dataset(\n",
    "    return_lesions_mask=True,\n",
    "    level='image',\n",
    "    max_lesion_diam_mm=1.,\n",
    "    extract_patches=False,\n",
    "    extract_patches_method='all',  # 'centered'\n",
    "    patch_size=256,\n",
    "    stride=256,\n",
    "    min_breast_fraction_roi=0.5,\n",
    "    normalize=None,\n",
    "    n_jobs=-1,\n",
    "    partitions=['train', 'val']\n",
    ")\n",
    "\n",
    "dehazing_params = {'omega': 0.9, 'window_size': 11, 'radius': 40, 'eps': 1e-5}\n",
    "\n",
    "hough1_params = {'method': cv2.HOUGH_GRADIENT, 'dp': 1, 'minDist': 20,\n",
    "                    'param1': 300, 'param2': 8,  'minRadius': 2, 'maxRadius': 20}\n",
    "\n",
    "hough2_params = {'method': cv2.HOUGH_GRADIENT, 'dp': 1, 'minDist': 20,\n",
    "                    'param1': 300, 'param2': 10,  'minRadius': 2, 'maxRadius': 20}\n",
    "back_ext_radius = 50\n",
    "erosion_iter = 20\n",
    "erosion_size = 5\n",
    "\n",
    "path = Path('/home/jseia/Desktop/ml-dl/data/hough')\n",
    "path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "hd = HoughCalcificationDetection(\n",
    "    dehazing_params, back_ext_radius,\n",
    "    path,\n",
    "    hough1_params, hough2_params,\n",
    "    erosion_iter=erosion_iter,\n",
    "    erosion_size=erosion_size\n",
    ")\n",
    "\n",
    "BASE_PATH = Path('../../../data_rois/')\n",
    "\n",
    "idx = 0\n",
    "case = db[idx]\n",
    "image = case['img']\n",
    "image_id = db.df.iloc[idx].img_id\n",
    "radiouses = case['radiuses']\n",
    "true_bboxes = db[idx]['lesion_bboxes']\n",
    "\n",
    "_, h2_circles = hd.detect(\n",
    "    image, image_id, load_processed_images=True, hough2=True)\n",
    "tp, fp, fn, gt_d, close_fp = get_tp_fp_fn(true_bboxes, radiouses, h2_circles, 7, 0.0)\n",
    "\n",
    "del h2_circles\n",
    "\n",
    "haarfe = HaarFeatureExtractor(14)\n",
    "tp_features = haarfe.extract_features(image, tp)\n",
    "tp_features['label'] = 'tp'\n",
    "# logging.info('Extracting features from FP')\n",
    "fp_features = haarfe.extract_features(\n",
    "    image, fp, haarfe.integral_image, haarfe.diagintegral_image)\n",
    "fp_features['label'] = 'fp'\n",
    "\n",
    "features = pd.concat([tp_features, fp_features], ignore_index=True)\n",
    "del fp_features, tp_features\n",
    "\n",
    "db_filename = BASE_PATH/'haar_features'/f'{image_id}_tp.fth'\n",
    "db_filename.parent.mkdir(exist_ok=True, parents=True)\n",
    "features.to_feather(db_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a95fdd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2421"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(haarfe.features_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805d8a1",
   "metadata": {},
   "source": [
    "### feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056020c6",
   "metadata": {},
   "source": [
    "Old things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0946decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dataset:\n",
    "#     def __init__(\n",
    "#         self, pos_df: pd.DataFrame, neg_df: pd.DataFrame,\n",
    "#         kr: float = 5, normalize: bool = False, seed: int = 42\n",
    "#     ):\n",
    "#         self.pos_df = pos_df\n",
    "#         self.neg_df = neg_df\n",
    "#         self.kr = kr\n",
    "#         self.nomalize = normalize\n",
    "#         self.seed = seed\n",
    "\n",
    "#         self.train_discarded_files = []\n",
    "#         self.val_discarded_files = []\n",
    "\n",
    "#         self.train_pos_cases_list = self.pos_df['filename'].sample(\n",
    "#             frac=0.5, replace=False, random_state=self.seed\n",
    "#         )\n",
    "#         self.train_pos_cases_list = self.train_pos_cases_list.tolist()\n",
    "#         condition = ~self.pos_df.filename.isin(self.train_pos_cases_list)\n",
    "#         self.val_pos_cases_list = self.pos_df.loc[condition, 'filename'].tolist()\n",
    "\n",
    "#         self.n_neg = len(self.train_pos_cases_list) * self.kr\n",
    "#         self.used_neg_files = []\n",
    "\n",
    "\n",
    "#     def open_img(self, filename: str):\n",
    "#         # filename = '/home/jseia/Desktop/ml-dl/data_rois/'+filename\n",
    "#         return cv2.imread(filename, cv2.IMREAD_ANYDEPTH)\n",
    "    \n",
    "#     def open_img_normalize(self, filename: str, mean: float, std: float):\n",
    "#         # TODO: add defaults mean and std\n",
    "#         return (cv2.imread(filename, cv2.IMREAD_ANYDEPTH) - mean) / std\n",
    "\n",
    "#     def load_imgs(self, pos_img_files: list, neg_img_files: list):\n",
    "#         # Imgs/features # TODO: Define if images or features\n",
    "#         xs = []\n",
    "#         # TODO: parallelize dataloading with multithread\n",
    "#         if self.normalize:\n",
    "#             xs.extend([self.open_img_normalize(f) for f in pos_img_files])\n",
    "#             xs.extend([self.open_img_normalize(f) for f in neg_img_files])\n",
    "#         else:\n",
    "#             xs.extend([self.open_img(f) for f in pos_img_files])\n",
    "#             xs.extend([self.open_img(f) for f in neg_img_files])\n",
    "        \n",
    "#         # Labels\n",
    "#         ys = np.zeros((len(pos_img_files)+len(neg_img_files)))\n",
    "#         ys[:len(pos_img_files)] = 1\n",
    "#         return np.array(xs), ys\n",
    "\n",
    "#     def get_train_batch(self, discarded_files: list = None):\n",
    "#         # Define the number of files to sample\n",
    "#         n_sample = self.n_neg if discarded_files is None else len(discarded_files)\n",
    "        \n",
    "#         # Discard rejected files\n",
    "#         self.train_neg_cases_list = \\\n",
    "#             [fn for fn in self.train_neg_cases_list if fn not in discarded_files]\n",
    "        \n",
    "#         # Define the cases from which to sample and sample\n",
    "#         condition = ~self.neg_df.filename.isin(self.used_neg_files),\n",
    "#         sample = self.neg_df.loc[condition, 'filename'].sample(\n",
    "#             n=n_sample, replace=False, random_state=self.seed\n",
    "#         )\n",
    "#         sample = sample.tolist()\n",
    "#         self.train_neg_cases_list.extend(sample)\n",
    "#         train_files_list = self.train_pos_cases_list + self.train_neg_cases_list\n",
    "\n",
    "#         # Keep track of history\n",
    "#         self.used_neg_files.extend(sample)\n",
    "        \n",
    "#         # Load the images/features # TODO: Define if images or features\n",
    "#         xs, ys = self.load_imgs(self.train_pos_cases_list, self.train_neg_cases_list)\n",
    "        \n",
    "#         return train_files_list, xs, ys\n",
    "\n",
    "#     def get_val_batch(self, discarded_files: list = None):\n",
    "#         # Define the number of files to sample\n",
    "#         n_sample = self.n_neg if discarded_files is None else len(discarded_files)\n",
    "        \n",
    "#         # Discard rejected files\n",
    "#         self.val_neg_cases_list = \\\n",
    "#             [fn for fn in self.val_neg_cases_list if fn not in discarded_files]\n",
    "        \n",
    "#         # Define the cases from which to sample and sample\n",
    "#         condition = ~self.neg_df.filename.isin(self.used_neg_files),\n",
    "#         sample = self.neg_df.loc[condition, 'filename'].sample(\n",
    "#             n=n_sample, replace=False, random_state=self.seed\n",
    "#         )\n",
    "#         sample = sample.tolist()\n",
    "#         self.val_neg_cases_list.extend(sample)\n",
    "#         val_files_list = self.val_pos_cases_list + self.val_neg_cases_list\n",
    "\n",
    "#         # Keep track of history\n",
    "#         self.used_neg_files.extend(sample)\n",
    "        \n",
    "#         # Load the images/features  # TODO: Define if images or features\n",
    "#         xs, ys = self.load_imgs(self.val_pos_cases_list, self.val_neg_cases_list)\n",
    "        \n",
    "#         return val_files_list, xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d061d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg = pd.read_csv('/home/jseia/Desktop/ml-dl/data_rois/bg.txt', header=None, nrows=100000)[0].tolist()\n",
    "# pos = pd.read_csv('/home/jseia/Desktop/ml-dl/data_rois/positives/info.dat', sep=' ', header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "334df173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ThresholdPolarity = tp.NamedTuple(\n",
    "#     'ThresholdPolarity', [('threshold', float), ('polarity', float)]\n",
    "# )\n",
    "\n",
    "# ClassifierResult = tp.NamedTuple(\n",
    "#     'ClassifierResult', [\n",
    "#         ('threshold', float), ('polarity', int), ('classification_error', float),\n",
    "#         ('classifier', tp.Callable[[np.ndarray], float])\n",
    "#     ])\n",
    "\n",
    "# WeakClassifier = tp.NamedTuple(\n",
    "#     'WeakClassifier', [\n",
    "#         ('threshold', float), ('polarity', int), ('alpha', float),\n",
    "#         ('classifier', tp.Callable[[np.ndarray], float])\n",
    "#     ])\n",
    "\n",
    "# PredictionStats = tp.NamedTuple(\n",
    "#     'PredictionStats', [('tn', int), ('fp', int), ('fn', int), ('tp', int)]\n",
    "# )\n",
    "\n",
    "\n",
    "# def prediction_stats(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[np.ndarray, PredictionStats]:\n",
    "#     c = confusion_matrix(y_true, y_pred)\n",
    "#     tn, fp, fn, tp = c.ravel()\n",
    "#     return c, PredictionStats(tn=tn, fp=fp, fn=fn, tp=tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "632b99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_face = np.sum(xs[ys > .5], axis=0) / xs[ys > .5].shape[0]\n",
    "# average_face = (average_face - average_face.min()) / (average_face.max() - average_face.min())\n",
    "# plt.figure()\n",
    "# plt.imshow((average_face), cmap='gray')\n",
    "\n",
    "# average_face = np.sum(xs[ys < .5], axis=0) / xs[ys < .5].shape[0]\n",
    "# average_face = (average_face - average_face.min()) / (average_face.max() - average_face.min())\n",
    "# plt.figure()\n",
    "# plt.imshow((average_face), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "f2da42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# #TODO: check cache\n",
    "# @njit\n",
    "# def weak_classifier(\n",
    "#     x: np.ndarray, f: fm.Feature, polarity: float, theta: float\n",
    "# ) -> float:\n",
    "#     # TODO: Check if Bria uses theta and polarity\n",
    "#     return (np.sign((polarity * theta) - (polarity * f(x))) + 1) // 2\n",
    "\n",
    "# @njit\n",
    "# def run_weak_classifier(x: np.ndarray, c: WeakClassifier) -> float:\n",
    "#     return weak_classifier(x=x, f=c.classifier, polarity=c.polarity, theta=c.threshold)\n",
    "\n",
    "# @njit\n",
    "# def strong_classifier(x: np.ndarray, weak_classifiers: tp.List[WeakClassifier]) -> int:\n",
    "#     sum_hypotheses = 0.\n",
    "#     # sum_alphas = 0.\n",
    "#     for c in weak_classifiers:\n",
    "#         sum_hypotheses += c.alpha * run_weak_classifier(x, c)\n",
    "#     #     sum_alphas += c.alpha\n",
    "#     # return 1 if (sum_hypotheses >= .5*sum_alphas) else 0\n",
    "#     return sum_hypotheses\n",
    "\n",
    "\n",
    "# @njit\n",
    "# def fast_rank_loss(preds: np.ndarray, labels: np.ndarray):\n",
    "#     idx = np.argsort(preds)\n",
    "#     labels = labels[idx]\n",
    "#     temp = np.cumsum(labels)\n",
    "#     count = temp[np.where(labels==0)].sum()\n",
    "#     return count\n",
    "\n",
    "# @njit\n",
    "# def get_rank_mat(preds: np.ndarray, labels: np.ndarray):\n",
    "#     pos_loc = np.where(labels, True, False)\n",
    "#     pos_preds = preds[pos_loc]\n",
    "#     neg_preds = preds[~pos_loc]\n",
    "#     n_pos_preds = len(pos_preds)\n",
    "#     n_neg_preds = len(neg_preds)\n",
    "#     pos_mat = pos_preds.repeat(n_neg_preds).reshape(n_pos_preds, n_neg_preds)\n",
    "#     neg_mat = neg_preds.repeat(n_pos_preds).reshape(n_neg_preds, n_pos_preds).T\n",
    "#     return np.where(pos_mat < neg_mat, 1, 0)\n",
    "\n",
    "# # TODO analyse combining this and previous funct\n",
    "# @njit\n",
    "# def weight_update(\n",
    "#     pos_preds: np.ndarray, neg_preds: np.ndarray, weights:np.ndarray, alpha: float\n",
    "# ):\n",
    "#     n_pos_preds = len(pos_preds)\n",
    "#     n_neg_preds = len(neg_preds)\n",
    "#     pos_mat = pos_preds.repeat(n_neg_preds).reshape(n_pos_preds, n_neg_preds)\n",
    "#     neg_mat = neg_preds.repeat(n_pos_preds).reshape(n_neg_preds, n_pos_preds).T\n",
    "#     return weights * np.exp(alpha * np.diff(neg_mat, pos_mat)) / weights.sum()\n",
    "\n",
    "# @njit\n",
    "# def wieghted_rank_loss(\n",
    "#     preds: np.ndarray, labels: np.ndarray, weights: np.ndarray\n",
    "# ):\n",
    "#     rank_mat = get_rank_mat(preds, labels)\n",
    "#     return (weights * rank_mat).sum()\n",
    "\n",
    "# @njit\n",
    "# def get_initial_alpha(\n",
    "#     pos_preds: np.ndarray, neg_preds: np.ndarray, weights: np.ndarray\n",
    "# ):\n",
    "#     n_pos_preds = len(pos_preds)\n",
    "#     n_neg_preds = len(neg_preds)\n",
    "#     pos_mat = pos_preds.repeat(n_neg_preds).reshape(n_pos_preds, n_neg_preds)\n",
    "#     neg_mat = neg_preds.repeat(n_pos_preds).reshape(n_neg_preds, n_pos_preds).T\n",
    "#     weighted_preds = weights * np.diff(neg_mat, pos_mat)\n",
    "#     return 1/2 * np.ln((1 + weighted_preds) / (1 - weighted_preds))\n",
    "\n",
    "# @njit\n",
    "# def get_pos_and_neg_preds_matrices(preds: np.ndarray, labels: np.ndarray):\n",
    "#     pos_loc = np.where(labels, True, False)\n",
    "#     pos_preds = preds[pos_loc]\n",
    "#     neg_preds = preds[~pos_loc]\n",
    "#     n_pos_preds = len(pos_preds)\n",
    "#     n_neg_preds = len(neg_preds)\n",
    "#     pos_mat = \\\n",
    "#         pos_preds.repeat(n_neg_preds).reshape(n_pos_preds, n_neg_preds)\n",
    "#     neg_mat = \\\n",
    "#         neg_preds.repeat(n_pos_preds).reshape(n_neg_preds, n_pos_preds).T\n",
    "#     return pos_mat, neg_mat\n",
    "    \n",
    "\n",
    "# @njit\n",
    "# def choose_weak(\n",
    "#     strong_class_pred: np.ndarray, x: np.ndarray, labels:np.ndarray,\n",
    "#     weights: np.ndarray, weak_classifiers: tp.List[WeakClassifier]\n",
    "# ):\n",
    "#     # strong preds\n",
    "#     pos_mat_strong, neg_mat_strong = \\\n",
    "#         get_pos_and_neg_preds_matrices(strong_class_pred, labels)\n",
    "#     acc = []\n",
    "#     for c in weak_classifiers:\n",
    "#         weak_pred = run_weak_classifier(x, c)\n",
    "#         pos_mat_weak, neg_mat_weak = \\\n",
    "#             get_pos_and_neg_preds_matrices(weak_pred, labels)\n",
    "#         rank_mat = (\n",
    "#             neg_mat_strong + c.alpha * neg_mat_weak >= \n",
    "#             pos_mat_strong + c.alpha * pos_mat_weak\n",
    "#         )\n",
    "#         acc.append(rank_mat * weights)\n",
    "#     return weak_classifiers[np.argmax(acc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def build_running_sums(ys: np.ndarray, ws: np.ndarray) -> Tuple[float, float, List[float], List[float]]:\n",
    "    s_minus, s_plus = 0., 0.\n",
    "    t_minus, t_plus = 0., 0.\n",
    "    s_minuses, s_pluses = [], []\n",
    "    \n",
    "    for y, w in zip(ys, ws):\n",
    "        if y < .5:\n",
    "            s_minus += w\n",
    "            t_minus += w\n",
    "        else:\n",
    "            s_plus += w\n",
    "            t_plus += w\n",
    "        s_minuses.append(s_minus)\n",
    "        s_pluses.append(s_plus)\n",
    "    return t_minus, t_plus, s_minuses, s_pluses\n",
    "\n",
    "\n",
    "@jit\n",
    "def find_best_threshold(zs: np.ndarray, t_minus: float, t_plus: float, s_minuses: List[float], s_pluses: List[float]) -> ThresholdPolarity:\n",
    "    min_e = float('inf')\n",
    "    min_z, polarity = 0, 0\n",
    "    for z, s_m, s_p in zip(zs, s_minuses, s_pluses):\n",
    "        error_1 = s_p + (t_minus - s_m)\n",
    "        error_2 = s_m + (t_plus - s_p)\n",
    "        if error_1 < min_e:\n",
    "            min_e = error_1\n",
    "            min_z = z\n",
    "            polarity = -1\n",
    "        elif error_2 < min_e:\n",
    "            min_e = error_2\n",
    "            min_z = z\n",
    "            polarity = 1\n",
    "    return ThresholdPolarity(threshold=min_z, polarity=polarity)\n",
    "\n",
    "\n",
    "def determine_threshold_polarity(ys: np.ndarray, ws: np.ndarray, zs: np.ndarray) -> ThresholdPolarity:  \n",
    "    # Sort according to score\n",
    "    p = np.argsort(zs)\n",
    "    zs, ys, ws = zs[p], ys[p], ws[p]\n",
    "    \n",
    "    # Determine the best threshold: build running sums\n",
    "    t_minus, t_plus, s_minuses, s_pluses = build_running_sums(ys, ws)\n",
    "    \n",
    "    # Determine the best threshold: select optimal threshold.\n",
    "    return find_best_threshold(zs, t_minus, t_plus, s_minuses, s_pluses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c99c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_EVERY     = 2000\n",
    "KEEP_PROBABILITY = 1./4.\n",
    "\n",
    "def build_weak_classifiers(\n",
    "    prefix: str, num_features: int, xis: np.ndarray, ys: np.ndarray, features: List[fm.Feature], ws: tp.Optional[np.ndarray] = None\n",
    ") -> Tuple[List[WeakClassifier], List[float]]:\n",
    "    \n",
    "    if ws is None:\n",
    "        # TODO: adapt to 2d weights\n",
    "        m = len(ys[ys < .5])  # number of negative example\n",
    "        l = len(ys[ys > .5])  # number of positive examples\n",
    "\n",
    "        # Initialize the weights\n",
    "        ws = np.zeros_like(ys)\n",
    "        ws[ys < .5] = 1./(2.*m)\n",
    "        ws[ys > .5] = 1./(2.*l)\n",
    "    \n",
    "    # Keep track of the history of the example weights.\n",
    "    w_history = [ws]\n",
    "\n",
    "    total_start_time = time.time()\n",
    "    with Parallel(n_jobs=-1, backend='threading') as parallel:\n",
    "        weak_classifiers = []  # type: List[WeakClassifier]\n",
    "        for t in range(num_features):\n",
    "            print(f'Building weak classifier {t+1}/{num_features} ...')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Normalize the weights\n",
    "            #TODO: ignore\n",
    "            ws = normalize_weights(ws)\n",
    "            \n",
    "            status_counter = STATUS_EVERY\n",
    "\n",
    "            # Select best weak classifier for this round\n",
    "            best = ClassifierResult(polarity=0, threshold=0, classification_error=float('inf'), classifier=None)\n",
    "            # TODO: add tqdm\n",
    "            for i, f in enumerate(features):\n",
    "                status_counter -= 1\n",
    "                improved = False\n",
    "\n",
    "                # Python runs singlethreaded. To speed things up,\n",
    "                # we're only anticipating every other feature, give or take.\n",
    "                # TODO: ignore?\n",
    "                if KEEP_PROBABILITY < 1.:\n",
    "                    skip_probability = np.random.random()\n",
    "                    if skip_probability > KEEP_PROBABILITY:\n",
    "                        continue\n",
    "\n",
    "                # TODO: avoid\n",
    "                result = apply_feature(f, xis, ys, ws, parallel)\n",
    "                \n",
    "                if result.classification_error < best.classification_error:\n",
    "                    improved = True\n",
    "                    best = result\n",
    "\n",
    "                # Print status every couple of iterations.\n",
    "                if improved or status_counter == 0:\n",
    "                    current_time = datetime.now()\n",
    "                    duration = current_time - start_time\n",
    "                    total_duration = current_time - total_start_time\n",
    "                    status_counter = STATUS_EVERY\n",
    "                    if improved:\n",
    "                        print(f't={t+1}/{num_features} {total_duration.total_seconds():.2f}s ({duration.total_seconds():.2f}s in this stage) {i+1}/{len(features)} {100*i/len(features):.2f}% evaluated. Classification error improved to {best.classification_error:.5f} using {str(best.classifier)} ...')\n",
    "                    else:\n",
    "                        print(f't={t+1}/{num_features} {total_duration.total_seconds():.2f}s ({duration.total_seconds():.2f}s in this stage) {i+1}/{len(features)} {100*i/len(features):.2f}% evaluated.')\n",
    "\n",
    "            # After the best classifier was found, determine alpha\n",
    "            beta = best.classification_error / (1 - best.classification_error)\n",
    "            alpha = np.log(1. / beta)\n",
    "            \n",
    "            # Build the weak classifier\n",
    "            classifier = WeakClassifier(threshold=best.threshold, polarity=best.polarity, classifier=best.classifier, alpha=alpha)\n",
    "            \n",
    "            # Update the weights for misclassified examples\n",
    "            for i, (x, y) in enumerate(zip(xis, ys)):\n",
    "                h = run_weak_classifier(x, classifier)\n",
    "                e = np.abs(h - y)\n",
    "                ws[i] = ws[i] * np.power(beta, 1-e)\n",
    "                \n",
    "            # Register this weak classifier           \n",
    "            weak_classifiers.append(classifier)\n",
    "            w_history.append(ws)\n",
    "        \n",
    "            pickle.dump(classifier, open(f'{prefix}-weak-learner-{t+1}-of-{num_features}.pickle', 'wb'))\n",
    "    \n",
    "    print(f'Done building {num_features} weak classifiers.')\n",
    "    return weak_classifiers, w_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faeb309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionStats(tn=1.0, fp=1.0, fn=1.0, tp=1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing as tp\n",
    "PredictionStats = tp.NamedTuple(\n",
    "    'PredictionStats', [('tn', int), ('fp', int), ('fn', int), ('tp', int)]\n",
    ")\n",
    "\n",
    "PredictionStats(*(np.ones(4).ravel()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f094a5cfaa0fb3b1e730adb1bd2a385aed1bdbc34460cd7c5bc1913497751e38"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('calc_det')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
