{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5894d963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9139/1242442907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../models/bria2014/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbria2014\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_instantiator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ml-dl/calc-det/notebooks/../database/dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/calc_det/lib/python3.9/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/calc_det/lib/python3.9/site-packages/pandas/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pandas._testing import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/calc_det/lib/python3.9/site-packages/pandas/_testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m \u001b[0mcython_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pyforest\n",
    "import SimpleITK as sitk\n",
    "import pydicom\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import sys; sys.path.insert(0, '../models/bria2014/')\n",
    "from database.dataset import *\n",
    "from models.bria2014.feature_extraction import feature_instantiator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a6fc9",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f7d3e",
   "metadata": {},
   "source": [
    "#### Check mayimum value in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d075b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcm_path = Path('/home/jseia/Desktop/ml-dl/calc-det/data/INbreast Release 1.0/AllDICOMs/')\n",
    "\n",
    "# max_val = []\n",
    "# for dcm in dcm_path.iterdir():\n",
    "#     if dcm.name.endswith('.dcm'):\n",
    "#         img = sitk.ReadImage(str(dcm))\n",
    "#         img_array = sitk.GetArrayFromImage(img)\n",
    "#         max_val.append(img_array.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08e5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcm_path = Path('/home/jseia/Desktop/ml-dl/calc-det/data/INbreast Release 1.0/AllDICOMs/')\n",
    "\n",
    "# bits_alloc = []\n",
    "# bits_stored = []\n",
    "# for dcm in dcm_path.iterdir():\n",
    "#     if dcm.name.endswith('.dcm'):\n",
    "#         img = pydicom.dcmread(str(dcm), stop_before_piyels=True)\n",
    "#         bits_alloc.append(img['BitsAllocated'].value)\n",
    "#         bits_stored.append(img['BitsStored'].value)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b015dde",
   "metadata": {},
   "source": [
    "#### Quatum noise tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4247ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def T(y, y_max=4095, T_max=65.535):\n",
    "#     return T_max*np.sqrt(y)/np.sqrt(y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c129d",
   "metadata": {},
   "source": [
    "### Detection stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85fdfbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = INBreast_Dataset(\n",
    "#     lesion_types = ['calcification'],\n",
    "#     return_lesions_mask = True,\n",
    "#     level = 'rois',\n",
    "#     partitions = ['train', 'test'],\n",
    "#     max_lesion_diam_mm = 1.0,\n",
    "#     extract_patches = False,\n",
    "#     extract_patches_method = 'centered',\n",
    "#     patch_size = 14,\n",
    "#     stride = 10,\n",
    "#     min_breast_fraction_roi = 0.9,\n",
    "#     n_jobs = 2,\n",
    "#     cropped_imgs = True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa47a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.df.loc[db.df.label == 'normal', 'filename'].to_csv('GFG.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b06cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_df = db.df.loc[db.df.label == 'abnormal']\n",
    "# pos_df['n'] = 1\n",
    "# pos_df['c1'] = 0\n",
    "# pos_df['c2'] = 0\n",
    "# pos_df['c3'] = 14\n",
    "# pos_df['c4'] = 14\n",
    "# pos_df.loc[:, ['filename', 'n', 'c1', 'c2', 'c3', 'c4']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eff8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.read_csv('/home/jseia/Desktop/ml-dl/data_rois/positives/info.dat', sep=' ', header=None)\n",
    "# # a[0] = 'positives/'+a[0]\n",
    "# a.head()\n",
    "# a.to_csv('/home/jseia/Desktop/ml-dl/data_rois/positives/info.dat', sep=' ', index=False, header=False)\n",
    "# a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd18da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[0] = a[0].str.strip('positives/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d3ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bg = pd.read_csv('/home/jseia/Desktop/ml-dl/data_rois/patches/bg.txt', header=None)\n",
    "# bg[0] = 'negatives/'+bg[0]\n",
    "# bg.to_csv('/home/jseia/Desktop/ml-dl/data_rois/patches/bg.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70dc9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fea3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, file_ in a[0].iteritems():\n",
    "#     base = Path('/home/jseia/Desktop/ml-dl/data_rois/positives')\n",
    "#     (base/file_.split('/')[0]).mkdir(parents=True, exist_ok=True)\n",
    "#     subprocess.call(['mv', f'/home/jseia/Desktop/ml-dl/data_rois/patches/{file_}', f'/home/jseia/Desktop/ml-dl/data_rois/positives/{file_}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04d10e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_df.loc[:, ['filename', 'n', 'c1', 'c2', 'c3', 'c4']].to_csv('info.csv', index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0fa66",
   "metadata": {},
   "source": [
    "test integral image times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4555b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import time\n",
    "\n",
    "# mean = []\n",
    "# for i in range(10000):\n",
    "#     start = time.time()\n",
    "#     to_integral(sample_image)\n",
    "#     mean.append(time.time()-start)\n",
    "\n",
    "# print(f'mean: {np.mean(mean)} +- std: {np.std(mean)}')\n",
    "\n",
    "# mean = []\n",
    "# for i in range(10000):\n",
    "#     start = time.time()\n",
    "#     integral(sample_image)\n",
    "#     mean.append(time.time()-start)\n",
    "\n",
    "# print(f'mean: {np.mean(mean)} +- std: {np.std(mean)}')\n",
    "\n",
    "# mean = []\n",
    "# for i in range(10000):\n",
    "#     start = time.time()\n",
    "#     diag_integral(sample_image)\n",
    "#     mean.append(time.time()-start)\n",
    "\n",
    "# print(f'mean: {np.mean(mean)} +- std: {np.std(mean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805d8a1",
   "metadata": {},
   "source": [
    "### feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest\n",
    "import SimpleITK as sitk\n",
    "import pydicom\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import sys; sys.path.insert(0, '../models/bria2014/')\n",
    "from database.dataset import *\n",
    "from models.bria2014.feature_extraction import feature_instantiator\n",
    "import pyforest\n",
    "import SimpleITK as sitk\n",
    "import pydicom\n",
    "from numba import njit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import sys; sys.path.insert(0, '../models/bria2014/')\n",
    "from database.dataset import *\n",
    "from models.bria2014.feature_extraction import feature_instantiator, to_diag_integral, to_integral\n",
    "import models.bria2014.feature_modules as fm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, confusion_matrix\n",
    "import typing as tp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# features = feature_instantiator(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "66e3c649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0946decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(\n",
    "        self, pos_df: pd.DataFrame, neg_df: pd.DataFrame,\n",
    "        kr: float = 5, normalize: bool = False, seed: int = 42\n",
    "    ):\n",
    "        self.pos_df = pos_df\n",
    "        self.neg_df = neg_df\n",
    "        self.kr = kr\n",
    "        self.nomalize = normalize\n",
    "        self.seed = seed\n",
    "\n",
    "        self.train_discarded_files = []\n",
    "        self.val_discarded_files = []\n",
    "\n",
    "        self.train_pos_cases_list = self.pos_df['filename'].sample(\n",
    "            frac=0.5, replace=False, random_state=self.seed\n",
    "        )\n",
    "        self.train_pos_cases_list = self.train_pos_cases_list.tolist()\n",
    "        condition = ~self.pos_df.filename.isin(self.train_pos_cases_list)\n",
    "        self.val_pos_cases_list = self.pos_df.loc[condition, 'filename'].tolist()\n",
    "\n",
    "        self.n_neg = len(self.train_pos_cases_list) * self.kr\n",
    "        self.used_neg_files = []\n",
    "\n",
    "\n",
    "    def open_img(self, filename: str):\n",
    "        # filename = '/home/jseia/Desktop/ml-dl/data_rois/'+filename\n",
    "        return cv2.imread(filename, cv2.IMREAD_ANYDEPTH)\n",
    "    \n",
    "    def open_img_normalize(self, filename: str, mean: float, std: float):\n",
    "        # TODO: add defaults mean and std\n",
    "        return (cv2.imread(filename, cv2.IMREAD_ANYDEPTH) - mean) / std\n",
    "\n",
    "    def load_imgs(self, pos_img_files: list, neg_img_files: list):\n",
    "        # Imgs/features # TODO: Define if images or features\n",
    "        xs = []\n",
    "        # TODO: parallelize dataloading with multithread\n",
    "        if self.normalize:\n",
    "            xs.extend([self.open_img_normalize(f) for f in pos_img_files])\n",
    "            xs.extend([self.open_img_normalize(f) for f in neg_img_files])\n",
    "        else:\n",
    "            xs.extend([self.open_img(f) for f in pos_img_files])\n",
    "            xs.extend([self.open_img(f) for f in neg_img_files])\n",
    "        \n",
    "        # Labels\n",
    "        ys = np.zeros((len(pos_img_files)+len(neg_img_files)))\n",
    "        ys[:len(pos_img_files)] = 1\n",
    "        return np.array(xs), ys\n",
    "\n",
    "    def get_train_batch(self, discarded_files: list = None):\n",
    "        # Define the number of files to sample\n",
    "        n_sample = self.n_neg if discarded_files is None else len(discarded_files)\n",
    "        \n",
    "        # Discard rejected files\n",
    "        self.train_neg_cases_list = \\\n",
    "            [fn for fn in self.train_neg_cases_list if fn not in discarded_files]\n",
    "        \n",
    "        # Define the cases from which to sample and sample\n",
    "        condition = ~self.neg_df.filename.isin(self.used_neg_files),\n",
    "        sample = self.neg_df.loc[condition, 'filename'].sample(\n",
    "            n=n_sample, replace=False, random_state=self.seed\n",
    "        )\n",
    "        sample = sample.tolist()\n",
    "        self.train_neg_cases_list.extend(sample)\n",
    "        train_files_list = self.train_pos_cases_list + self.train_neg_cases_list\n",
    "\n",
    "        # Keep track of history\n",
    "        self.used_neg_files.extend(sample)\n",
    "        \n",
    "        # Load the images/features # TODO: Define if images or features\n",
    "        xs, ys = self.load_imgs(self.train_pos_cases_list, self.train_neg_cases_list)\n",
    "        \n",
    "        return train_files_list, xs, ys\n",
    "\n",
    "    def get_val_batch(self, discarded_files: list = None):\n",
    "        # Define the number of files to sample\n",
    "        n_sample = self.n_neg if discarded_files is None else len(discarded_files)\n",
    "        \n",
    "        # Discard rejected files\n",
    "        self.val_neg_cases_list = \\\n",
    "            [fn for fn in self.val_neg_cases_list if fn not in discarded_files]\n",
    "        \n",
    "        # Define the cases from which to sample and sample\n",
    "        condition = ~self.neg_df.filename.isin(self.used_neg_files),\n",
    "        sample = self.neg_df.loc[condition, 'filename'].sample(\n",
    "            n=n_sample, replace=False, random_state=self.seed\n",
    "        )\n",
    "        sample = sample.tolist()\n",
    "        self.val_neg_cases_list.extend(sample)\n",
    "        val_files_list = self.val_pos_cases_list + self.val_neg_cases_list\n",
    "\n",
    "        # Keep track of history\n",
    "        self.used_neg_files.extend(sample)\n",
    "        \n",
    "        # Load the images/features  # TODO: Define if images or features\n",
    "        xs, ys = self.load_imgs(self.val_pos_cases_list, self.val_neg_cases_list)\n",
    "        \n",
    "        return val_files_list, xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d061d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = pd.read_csv('/home/jseia/Desktop/ml-dl/data_rois/bg.txt', header=None, nrows=100000)[0].tolist()\n",
    "pos = pd.read_csv('/home/jseia/Desktop/ml-dl/data_rois/positives/info.dat', sep=' ', header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "334df173",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThresholdPolarity = tp.NamedTuple(\n",
    "    'ThresholdPolarity', [('threshold', float), ('polarity', float)]\n",
    ")\n",
    "\n",
    "ClassifierResult = tp.NamedTuple(\n",
    "    'ClassifierResult', [\n",
    "        ('threshold', float), ('polarity', int), ('classification_error', float),\n",
    "        ('classifier', tp.Callable[[np.ndarray], float])\n",
    "    ])\n",
    "\n",
    "WeakClassifier = tp.NamedTuple(\n",
    "    'WeakClassifier', [\n",
    "        ('threshold', float), ('polarity', int), ('alpha', float),\n",
    "        ('classifier', tp.Callable[[np.ndarray], float])\n",
    "    ])\n",
    "\n",
    "PredictionStats = tp.NamedTuple(\n",
    "    'PredictionStats', [('tn', int), ('fp', int), ('fn', int), ('tp', int)]\n",
    ")\n",
    "\n",
    "\n",
    "def prediction_stats(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[np.ndarray, PredictionStats]:\n",
    "    c = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = c.ravel()\n",
    "    return c, PredictionStats(tn=tn, fp=fp, fn=fn, tp=tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "632b99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_face = np.sum(xs[ys > .5], axis=0) / xs[ys > .5].shape[0]\n",
    "# average_face = (average_face - average_face.min()) / (average_face.max() - average_face.min())\n",
    "# plt.figure()\n",
    "# plt.imshow((average_face), cmap='gray')\n",
    "\n",
    "# average_face = np.sum(xs[ys < .5], axis=0) / xs[ys < .5].shape[0]\n",
    "# average_face = (average_face - average_face.min()) / (average_face.max() - average_face.min())\n",
    "# plt.figure()\n",
    "# plt.imshow((average_face), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "f2da42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# #TODO: check cache\n",
    "# @njit\n",
    "# def weak_classifier(\n",
    "#     x: np.ndarray, f: fm.Feature, polarity: float, theta: float\n",
    "# ) -> float:\n",
    "#     # TODO: Check if Bria uses theta and polarity\n",
    "#     return (np.sign((polarity * theta) - (polarity * f(x))) + 1) // 2\n",
    "\n",
    "# @njit\n",
    "# def run_weak_classifier(x: np.ndarray, c: WeakClassifier) -> float:\n",
    "#     return weak_classifier(x=x, f=c.classifier, polarity=c.polarity, theta=c.threshold)\n",
    "\n",
    "# @njit\n",
    "# def strong_classifier(x: np.ndarray, weak_classifiers: tp.List[WeakClassifier]) -> int:\n",
    "#     sum_hypotheses = 0.\n",
    "#     # sum_alphas = 0.\n",
    "#     for c in weak_classifiers:\n",
    "#         sum_hypotheses += c.alpha * run_weak_classifier(x, c)\n",
    "#     #     sum_alphas += c.alpha\n",
    "#     # return 1 if (sum_hypotheses >= .5*sum_alphas) else 0\n",
    "#     return sum_hypotheses\n",
    "\n",
    "\n",
    "# @njit\n",
    "# def fast_rank_loss(preds: np.ndarray, labels: np.ndarray):\n",
    "#     idx = np.argsort(preds)\n",
    "#     labels = labels[idx]\n",
    "#     temp = np.cumsum(labels)\n",
    "#     count = temp[np.where(labels==0)].sum()\n",
    "#     return count\n",
    "\n",
    "# @njit\n",
    "# def get_rank_mat(preds: np.ndarray, labels: np.ndarray):\n",
    "#     pos_loc = np.where(labels, True, False)\n",
    "#     pos_preds = preds[pos_loc]\n",
    "#     neg_preds = preds[~pos_loc]\n",
    "#     n_pos_preds = len(pos_preds)\n",
    "#     n_neg_preds = len(neg_preds)\n",
    "#     pos_mat = pos_preds.repeat(n_neg_preds).reshape(n_pos_preds, n_neg_preds)\n",
    "#     neg_mat = neg_preds.repeat(n_pos_preds).reshape(n_neg_preds, n_pos_preds).T\n",
    "#     return np.where(pos_mat < neg_mat, 1, 0)\n",
    "\n",
    "# # TODO analyse combining this and previous funct\n",
    "# @njit\n",
    "# def weight_update(\n",
    "#     pos_preds: np.ndarray, neg_preds: np.ndarray, weights:np.ndarray, alpha: float\n",
    "# ):\n",
    "#     n_pos_preds = len(pos_preds)\n",
    "#     n_neg_preds = len(neg_preds)\n",
    "#     pos_mat = pos_preds.repeat(n_neg_preds).reshape(n_pos_preds, n_neg_preds)\n",
    "#     neg_mat = neg_preds.repeat(n_pos_preds).reshape(n_neg_preds, n_pos_preds).T\n",
    "#     return weights * np.exp(alpha * np.diff(neg_mat, pos_mat)) / weights.sum()\n",
    "\n",
    "# @njit\n",
    "# def wieghted_rank_loss(\n",
    "#     preds: np.ndarray, labels: np.ndarray, weights: np.ndarray\n",
    "# ):\n",
    "#     rank_mat = get_rank_mat(preds, labels)\n",
    "#     return (weights * rank_mat).sum()\n",
    "\n",
    "# @njit\n",
    "# def get_initial_alpha(\n",
    "#     pos_preds: np.ndarray, neg_preds: np.ndarray, weights: np.ndarray\n",
    "# ):\n",
    "#     n_pos_preds = len(pos_preds)\n",
    "#     n_neg_preds = len(neg_preds)\n",
    "#     pos_mat = pos_preds.repeat(n_neg_preds).reshape(n_pos_preds, n_neg_preds)\n",
    "#     neg_mat = neg_preds.repeat(n_pos_preds).reshape(n_neg_preds, n_pos_preds).T\n",
    "#     weighted_preds = weights * np.diff(neg_mat, pos_mat)\n",
    "#     return 1/2 * np.ln((1 + weighted_preds) / (1 - weighted_preds))\n",
    "\n",
    "# @njit\n",
    "# def get_pos_and_neg_preds_matrices(preds: np.ndarray, labels: np.ndarray):\n",
    "#     pos_loc = np.where(labels, True, False)\n",
    "#     pos_preds = preds[pos_loc]\n",
    "#     neg_preds = preds[~pos_loc]\n",
    "#     n_pos_preds = len(pos_preds)\n",
    "#     n_neg_preds = len(neg_preds)\n",
    "#     pos_mat = \\\n",
    "#         pos_preds.repeat(n_neg_preds).reshape(n_pos_preds, n_neg_preds)\n",
    "#     neg_mat = \\\n",
    "#         neg_preds.repeat(n_pos_preds).reshape(n_neg_preds, n_pos_preds).T\n",
    "#     return pos_mat, neg_mat\n",
    "    \n",
    "\n",
    "# @njit\n",
    "# def choose_weak(\n",
    "#     strong_class_pred: np.ndarray, x: np.ndarray, labels:np.ndarray,\n",
    "#     weights: np.ndarray, weak_classifiers: tp.List[WeakClassifier]\n",
    "# ):\n",
    "#     # strong preds\n",
    "#     pos_mat_strong, neg_mat_strong = \\\n",
    "#         get_pos_and_neg_preds_matrices(strong_class_pred, labels)\n",
    "#     acc = []\n",
    "#     for c in weak_classifiers:\n",
    "#         weak_pred = run_weak_classifier(x, c)\n",
    "#         pos_mat_weak, neg_mat_weak = \\\n",
    "#             get_pos_and_neg_preds_matrices(weak_pred, labels)\n",
    "#         rank_mat = (\n",
    "#             neg_mat_strong + c.alpha * neg_mat_weak >= \n",
    "#             pos_mat_strong + c.alpha * pos_mat_weak\n",
    "#         )\n",
    "#         acc.append(rank_mat * weights)\n",
    "#     return weak_classifiers[np.argmax(acc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def build_running_sums(ys: np.ndarray, ws: np.ndarray) -> Tuple[float, float, List[float], List[float]]:\n",
    "    s_minus, s_plus = 0., 0.\n",
    "    t_minus, t_plus = 0., 0.\n",
    "    s_minuses, s_pluses = [], []\n",
    "    \n",
    "    for y, w in zip(ys, ws):\n",
    "        if y < .5:\n",
    "            s_minus += w\n",
    "            t_minus += w\n",
    "        else:\n",
    "            s_plus += w\n",
    "            t_plus += w\n",
    "        s_minuses.append(s_minus)\n",
    "        s_pluses.append(s_plus)\n",
    "    return t_minus, t_plus, s_minuses, s_pluses\n",
    "\n",
    "\n",
    "@jit\n",
    "def find_best_threshold(zs: np.ndarray, t_minus: float, t_plus: float, s_minuses: List[float], s_pluses: List[float]) -> ThresholdPolarity:\n",
    "    min_e = float('inf')\n",
    "    min_z, polarity = 0, 0\n",
    "    for z, s_m, s_p in zip(zs, s_minuses, s_pluses):\n",
    "        error_1 = s_p + (t_minus - s_m)\n",
    "        error_2 = s_m + (t_plus - s_p)\n",
    "        if error_1 < min_e:\n",
    "            min_e = error_1\n",
    "            min_z = z\n",
    "            polarity = -1\n",
    "        elif error_2 < min_e:\n",
    "            min_e = error_2\n",
    "            min_z = z\n",
    "            polarity = 1\n",
    "    return ThresholdPolarity(threshold=min_z, polarity=polarity)\n",
    "\n",
    "\n",
    "def determine_threshold_polarity(ys: np.ndarray, ws: np.ndarray, zs: np.ndarray) -> ThresholdPolarity:  \n",
    "    # Sort according to score\n",
    "    p = np.argsort(zs)\n",
    "    zs, ys, ws = zs[p], ys[p], ws[p]\n",
    "    \n",
    "    # Determine the best threshold: build running sums\n",
    "    t_minus, t_plus, s_minuses, s_pluses = build_running_sums(ys, ws)\n",
    "    \n",
    "    # Determine the best threshold: select optimal threshold.\n",
    "    return find_best_threshold(zs, t_minus, t_plus, s_minuses, s_pluses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c99c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_EVERY     = 2000\n",
    "KEEP_PROBABILITY = 1./4.\n",
    "\n",
    "def build_weak_classifiers(\n",
    "    prefix: str, num_features: int, xis: np.ndarray, ys: np.ndarray, features: List[fm.Feature], ws: tp.Optional[np.ndarray] = None\n",
    ") -> Tuple[List[WeakClassifier], List[float]]:\n",
    "    \n",
    "    if ws is None:\n",
    "        # TODO: adapt to 2d weights\n",
    "        m = len(ys[ys < .5])  # number of negative example\n",
    "        l = len(ys[ys > .5])  # number of positive examples\n",
    "\n",
    "        # Initialize the weights\n",
    "        ws = np.zeros_like(ys)\n",
    "        ws[ys < .5] = 1./(2.*m)\n",
    "        ws[ys > .5] = 1./(2.*l)\n",
    "    \n",
    "    # Keep track of the history of the example weights.\n",
    "    w_history = [ws]\n",
    "\n",
    "    total_start_time = time.time()\n",
    "    with Parallel(n_jobs=-1, backend='threading') as parallel:\n",
    "        weak_classifiers = []  # type: List[WeakClassifier]\n",
    "        for t in range(num_features):\n",
    "            print(f'Building weak classifier {t+1}/{num_features} ...')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Normalize the weights\n",
    "            #TODO: ignore\n",
    "            ws = normalize_weights(ws)\n",
    "            \n",
    "            status_counter = STATUS_EVERY\n",
    "\n",
    "            # Select best weak classifier for this round\n",
    "            best = ClassifierResult(polarity=0, threshold=0, classification_error=float('inf'), classifier=None)\n",
    "            # TODO: add tqdm\n",
    "            for i, f in enumerate(features):\n",
    "                status_counter -= 1\n",
    "                improved = False\n",
    "\n",
    "                # Python runs singlethreaded. To speed things up,\n",
    "                # we're only anticipating every other feature, give or take.\n",
    "                # TODO: ignore?\n",
    "                if KEEP_PROBABILITY < 1.:\n",
    "                    skip_probability = np.random.random()\n",
    "                    if skip_probability > KEEP_PROBABILITY:\n",
    "                        continue\n",
    "\n",
    "                # TODO: avoid\n",
    "                result = apply_feature(f, xis, ys, ws, parallel)\n",
    "                \n",
    "                if result.classification_error < best.classification_error:\n",
    "                    improved = True\n",
    "                    best = result\n",
    "\n",
    "                # Print status every couple of iterations.\n",
    "                if improved or status_counter == 0:\n",
    "                    current_time = datetime.now()\n",
    "                    duration = current_time - start_time\n",
    "                    total_duration = current_time - total_start_time\n",
    "                    status_counter = STATUS_EVERY\n",
    "                    if improved:\n",
    "                        print(f't={t+1}/{num_features} {total_duration.total_seconds():.2f}s ({duration.total_seconds():.2f}s in this stage) {i+1}/{len(features)} {100*i/len(features):.2f}% evaluated. Classification error improved to {best.classification_error:.5f} using {str(best.classifier)} ...')\n",
    "                    else:\n",
    "                        print(f't={t+1}/{num_features} {total_duration.total_seconds():.2f}s ({duration.total_seconds():.2f}s in this stage) {i+1}/{len(features)} {100*i/len(features):.2f}% evaluated.')\n",
    "\n",
    "            # After the best classifier was found, determine alpha\n",
    "            beta = best.classification_error / (1 - best.classification_error)\n",
    "            alpha = np.log(1. / beta)\n",
    "            \n",
    "            # Build the weak classifier\n",
    "            classifier = WeakClassifier(threshold=best.threshold, polarity=best.polarity, classifier=best.classifier, alpha=alpha)\n",
    "            \n",
    "            # Update the weights for misclassified examples\n",
    "            for i, (x, y) in enumerate(zip(xis, ys)):\n",
    "                h = run_weak_classifier(x, classifier)\n",
    "                e = np.abs(h - y)\n",
    "                ws[i] = ws[i] * np.power(beta, 1-e)\n",
    "                \n",
    "            # Register this weak classifier           \n",
    "            weak_classifiers.append(classifier)\n",
    "            w_history.append(ws)\n",
    "        \n",
    "            pickle.dump(classifier, open(f'{prefix}-weak-learner-{t+1}-of-{num_features}.pickle', 'wb'))\n",
    "    \n",
    "    print(f'Done building {num_features} weak classifiers.')\n",
    "    return weak_classifiers, w_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faeb309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionStats(tn=1.0, fp=1.0, fn=1.0, tp=1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing as tp\n",
    "PredictionStats = tp.NamedTuple(\n",
    "    'PredictionStats', [('tn', int), ('fp', int), ('fn', int), ('tp', int)]\n",
    ")\n",
    "\n",
    "PredictionStats(*(np.ones(4).ravel()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
